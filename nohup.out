Running train_bert_classfier.py with arguments GroNLP/bert-base-dutch-cased and CivilRights
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7002

Best accuracy=0.7002398081534772
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7014

Best accuracy=0.7014388489208633
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.6906

Best accuracy=0.7014388489208633
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.6787

Best accuracy=0.7014388489208633
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7230

Epoch 2/2

Validation Accuracy: 0.7638

Best accuracy=0.7637889688249401
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7362

Epoch 2/2

Validation Accuracy: 0.7686

Best accuracy=0.7685851318944844
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7038

Epoch 2/2

Validation Accuracy: 0.7710

Best accuracy=0.7709832134292566
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7302

Epoch 2/2

Validation Accuracy: 0.7890

Best accuracy=0.7889688249400479
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7422

Epoch 2/3

Validation Accuracy: 0.7878

Epoch 3/3

Validation Accuracy: 0.7818

Best accuracy=0.7889688249400479
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7446

Epoch 2/3

Validation Accuracy: 0.7794

Epoch 3/3

Validation Accuracy: 0.7890

Best accuracy=0.7889688249400479
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7554

Epoch 2/3

Validation Accuracy: 0.7818

Epoch 3/3

Validation Accuracy: 0.7746

Best accuracy=0.7889688249400479
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7530

Epoch 2/3

Validation Accuracy: 0.7866

Epoch 3/3

Validation Accuracy: 0.7902

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7398

Epoch 2/4

Validation Accuracy: 0.7878

Epoch 3/4

Validation Accuracy: 0.7854

Epoch 4/4

Validation Accuracy: 0.7806

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7218

Epoch 2/4

Validation Accuracy: 0.7686

Epoch 3/4

Validation Accuracy: 0.7698

Epoch 4/4

Validation Accuracy: 0.7818

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7398

Epoch 2/4

Validation Accuracy: 0.7782

Epoch 3/4

Validation Accuracy: 0.7854

Epoch 4/4

Validation Accuracy: 0.7866

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7446

Epoch 2/4

Validation Accuracy: 0.7734

Epoch 3/4

Validation Accuracy: 0.7938

Epoch 4/4

Validation Accuracy: 0.7866

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.6751

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.6451

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.6583

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.6847

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.6882

Epoch 2/2

Validation Accuracy: 0.7482

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7014

Epoch 2/2

Validation Accuracy: 0.7554

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7158

Epoch 2/2

Validation Accuracy: 0.7710

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7242

Epoch 2/2

Validation Accuracy: 0.7650

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7026

Epoch 2/3

Validation Accuracy: 0.7722

Epoch 3/3

Validation Accuracy: 0.7878

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7314

Epoch 2/3

Validation Accuracy: 0.7794

Epoch 3/3

Validation Accuracy: 0.7854

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.6978

Epoch 2/3

Validation Accuracy: 0.7770

Epoch 3/3

Validation Accuracy: 0.7842

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.6894

Epoch 2/3

Validation Accuracy: 0.7710

Epoch 3/3

Validation Accuracy: 0.7866

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7026

Epoch 2/4

Validation Accuracy: 0.7638

Epoch 3/4

Validation Accuracy: 0.7770

Epoch 4/4

Validation Accuracy: 0.7830

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7290

Epoch 2/4

Validation Accuracy: 0.7674

Epoch 3/4

Validation Accuracy: 0.7854

Epoch 4/4

Validation Accuracy: 0.7794

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7122

Epoch 2/4

Validation Accuracy: 0.7722

Epoch 3/4

Validation Accuracy: 0.7854

Epoch 4/4

Validation Accuracy: 0.7806

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7410

Epoch 2/4

Validation Accuracy: 0.7590

Epoch 3/4

Validation Accuracy: 0.7818

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 4/4

Validation Accuracy: 0.7770

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7158

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7302

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7446

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7194

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7326

Epoch 2/2

Validation Accuracy: 0.7818

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7398

Epoch 2/2

Validation Accuracy: 0.7674

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7590

Epoch 2/2

Validation Accuracy: 0.7902

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7482

Epoch 2/2

Validation Accuracy: 0.7794

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7554

Epoch 2/3

Validation Accuracy: 0.7890

Epoch 3/3

Validation Accuracy: 0.7902

Best accuracy=0.790167865707434
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7254

Epoch 2/3

Validation Accuracy: 0.7878

Epoch 3/3

Validation Accuracy: 0.7914

Best accuracy=0.7913669064748201
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7422

Epoch 2/3

Validation Accuracy: 0.7794

Epoch 3/3

Validation Accuracy: 0.7866

Best accuracy=0.7913669064748201
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7518

Epoch 2/3

Validation Accuracy: 0.7746

Epoch 3/3

Validation Accuracy: 0.7890

Best accuracy=0.7913669064748201
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7158

Epoch 2/4

Validation Accuracy: 0.7650

Epoch 3/4

Validation Accuracy: 0.7866

Epoch 4/4

Validation Accuracy: 0.8022

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.6679

Epoch 2/4

Validation Accuracy: 0.7830

Epoch 3/4

Validation Accuracy: 0.7878

Epoch 4/4

Validation Accuracy: 0.7890

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7422

Epoch 2/4

Validation Accuracy: 0.7830

Epoch 3/4

Validation Accuracy: 0.7830

Epoch 4/4

Validation Accuracy: 0.7938

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7338

Epoch 2/4

Validation Accuracy: 0.7734

Epoch 3/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.7818

Epoch 4/4

Validation Accuracy: 0.7914

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7110

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7170

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7074

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7170

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7122

Epoch 2/2

Validation Accuracy: 0.7650

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7350

Epoch 2/2

Validation Accuracy: 0.7854

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7362

Epoch 2/2

Validation Accuracy: 0.7734

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7446

Epoch 2/2

Validation Accuracy: 0.7734

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7218

Epoch 2/3

Validation Accuracy: 0.7746

Epoch 3/3

Validation Accuracy: 0.7914

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7482

Epoch 2/3

Validation Accuracy: 0.7866

Epoch 3/3

Validation Accuracy: 0.7986

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7302

Epoch 2/3

Validation Accuracy: 0.7638

Epoch 3/3

Validation Accuracy: 0.7806

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7566

Epoch 2/3

Validation Accuracy: 0.7818

Epoch 3/3

Validation Accuracy: 0.7842

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7494

Epoch 2/4

Validation Accuracy: 0.7662

Epoch 3/4

Validation Accuracy: 0.7662

Epoch 4/4

Validation Accuracy: 0.7770

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7434

Epoch 2/4

Validation Accuracy: 0.7770

Epoch 3/4

Validation Accuracy: 0.7986

Epoch 4/4

Validation Accuracy: 0.7986

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7158

Epoch 2/4

Validation Accuracy: 0.7794

Epoch 3/4

Validation Accuracy: 0.7722

Epoch 4/4

Validation Accuracy: 0.7794

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7302

Epoch 2/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.7662

Epoch 3/4

Validation Accuracy: 0.7866

Epoch 4/4

Validation Accuracy: 0.7854

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7506

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7362

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7458

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7494

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7314

Epoch 2/2

Validation Accuracy: 0.7758

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7434

Epoch 2/2

Validation Accuracy: 0.7902

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7182

Epoch 2/2

Validation Accuracy: 0.7830

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7566

Epoch 2/2

Validation Accuracy: 0.7662

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7494

Epoch 2/3

Validation Accuracy: 0.7710

Epoch 3/3

Validation Accuracy: 0.7998

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7410

Epoch 2/3

Validation Accuracy: 0.7638

Epoch 3/3

Validation Accuracy: 0.7794

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7158

Epoch 2/3

Validation Accuracy: 0.7698

Epoch 3/3

Validation Accuracy: 0.7866

Best accuracy=0.802158273381295
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7530

Epoch 2/3

Validation Accuracy: 0.7902

Epoch 3/3

Validation Accuracy: 0.8094

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7842

Epoch 2/4

Validation Accuracy: 0.7878

Epoch 3/4

Validation Accuracy: 0.7818

Epoch 4/4

Validation Accuracy: 0.7938

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7302

Epoch 2/4

Validation Accuracy: 0.7926

Epoch 3/4

Validation Accuracy: 0.7758

Epoch 4/4

Validation Accuracy: 0.7914

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7374

Epoch 2/4

Validation Accuracy: 0.7626

Epoch 3/4

Validation Accuracy: 0.7914

Epoch 4/4

Validation Accuracy: 0.7902

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.7734

Epoch 2/4

Validation Accuracy: 0.7818

Epoch 3/4

Validation Accuracy: 0.7890

Epoch 4/4

Validation Accuracy: 0.7926

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7578

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7014

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7362

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7434

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7386

Epoch 2/2

Validation Accuracy: 0.7770

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7578

Epoch 2/2

Validation Accuracy: 0.7818

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7410

Epoch 2/2

Validation Accuracy: 0.7674

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7554

Epoch 2/2

Validation Accuracy: 0.7770

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7446

Epoch 2/3

Validation Accuracy: 0.7662

Epoch 3/3

Validation Accuracy: 0.7734

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7242

Epoch 2/3

Validation Accuracy: 0.7854

Epoch 3/3

Validation Accuracy: 0.7926

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7566

Epoch 2/3

Validation Accuracy: 0.7794

Epoch 3/3

Validation Accuracy: 0.7926

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7242

Epoch 2/3

Validation Accuracy: 0.7638

Epoch 3/3

Validation Accuracy: 0.7842

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7422

Epoch 2/4

Validation Accuracy: 0.7914

Epoch 3/4

Validation Accuracy: 0.7818

Epoch 4/4

Validation Accuracy: 0.7902

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7398

Epoch 2/4

Validation Accuracy: 0.7758

Epoch 3/4

Validation Accuracy: 0.7890

Epoch 4/4

Validation Accuracy: 0.7902

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7170

Epoch 2/4

Validation Accuracy: 0.7866

Epoch 3/4

Validation Accuracy: 0.7914

Epoch 4/4

Validation Accuracy: 0.7890

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7206

Epoch 2/4

Validation Accuracy: 0.7818

Epoch 3/4

Validation Accuracy: 0.7758

Epoch 4/4

Validation Accuracy: 0.7854

Best accuracy=0.8093525179856115
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
Saving best performing model...
Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 180, in <module>
    torch.save(best_model_state, Path(f"src/classifier/models/bert_{bert_model_name}_{topic}_classifier.pth"))
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 716, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 687, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory src/classifier/models/bert_GroNLP does not exist.
Running train_bert_classfier.py with arguments GroNLP/bert-base-dutch-cased and Environment
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7592

Best accuracy=0.7591836734693878
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7429

Best accuracy=0.7591836734693878
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7561

Best accuracy=0.7591836734693878
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7653

Best accuracy=0.7653061224489796
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7663

Epoch 2/2

Validation Accuracy: 0.7857

Best accuracy=0.7857142857142857
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7551

Epoch 2/2

Validation Accuracy: 0.7755

Best accuracy=0.7857142857142857
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7643

Epoch 2/2

Validation Accuracy: 0.7806

Best accuracy=0.7857142857142857
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7561

Epoch 2/2

Validation Accuracy: 0.7857

Best accuracy=0.7857142857142857
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7449

Epoch 2/3

Validation Accuracy: 0.7816

Epoch 3/3

Validation Accuracy: 0.7857

Best accuracy=0.7857142857142857
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7704

Epoch 2/3

Validation Accuracy: 0.7724

Epoch 3/3

Validation Accuracy: 0.7776

Best accuracy=0.7857142857142857
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7765

Epoch 2/3

Validation Accuracy: 0.7878

Epoch 3/3

Validation Accuracy: 0.7918

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7673

Epoch 2/3

Validation Accuracy: 0.7755

Epoch 3/3

Validation Accuracy: 0.7867

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7602

Epoch 2/4

Validation Accuracy: 0.7929

Epoch 3/4

Validation Accuracy: 0.7745

Epoch 4/4

Validation Accuracy: 0.7857

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7633

Epoch 2/4

Validation Accuracy: 0.7806

Epoch 3/4

Validation Accuracy: 0.7878

Epoch 4/4

Validation Accuracy: 0.7827

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7765

Epoch 2/4

Validation Accuracy: 0.7806

Epoch 3/4

Validation Accuracy: 0.7980

Epoch 4/4

Validation Accuracy: 0.7918

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7622

Epoch 2/4

Validation Accuracy: 0.7837

Epoch 3/4

Validation Accuracy: 0.7888

Epoch 4/4

Validation Accuracy: 0.7898

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7327

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7367

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7276

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7184

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7551

Epoch 2/2

Validation Accuracy: 0.7796

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7541

Epoch 2/2

Validation Accuracy: 0.7735

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7459

Epoch 2/2

Validation Accuracy: 0.7898

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7602

Epoch 2/2

Validation Accuracy: 0.7888

Best accuracy=0.7918367346938775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7765

Epoch 2/3

Validation Accuracy: 0.7776

Epoch 3/3

Validation Accuracy: 0.7929

Best accuracy=0.7928571428571428
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7541

Epoch 2/3

Validation Accuracy: 0.7816

Epoch 3/3

Validation Accuracy: 0.7857

Best accuracy=0.7928571428571428
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7612

Epoch 2/3

Validation Accuracy: 0.7806

Epoch 3/3

Validation Accuracy: 0.7837

Best accuracy=0.7928571428571428
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7531

Epoch 2/3

Validation Accuracy: 0.7898

Epoch 3/3

Validation Accuracy: 0.7796

Best accuracy=0.7928571428571428
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7500

Epoch 2/4

Validation Accuracy: 0.7867

Epoch 3/4

Validation Accuracy: 0.7847

Epoch 4/4

Validation Accuracy: 0.7816

Best accuracy=0.7928571428571428
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7571

Epoch 2/4

Validation Accuracy: 0.7827

Epoch 3/4

Validation Accuracy: 0.7867

Epoch 4/4

Validation Accuracy: 0.7969

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7694

Epoch 2/4

Validation Accuracy: 0.7724

Epoch 3/4

Validation Accuracy: 0.7806

Epoch 4/4

Validation Accuracy: 0.7888

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7622

Epoch 2/4

Validation Accuracy: 0.7888

Epoch 3/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.7857

Epoch 4/4

Validation Accuracy: 0.7857

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7633

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7592

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7602

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7684

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7796

Epoch 2/2

Validation Accuracy: 0.7867

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7714

Epoch 2/2

Validation Accuracy: 0.7816

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7857

Epoch 2/2

Validation Accuracy: 0.7908

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7745

Epoch 2/2

Validation Accuracy: 0.7898

Best accuracy=0.7969387755102041
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=16
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7520

Epoch 2/3

Validation Accuracy: 0.7847

Epoch 3/3

Validation Accuracy: 0.8031

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7694

Epoch 2/3

Validation Accuracy: 0.7847

Epoch 3/3

Validation Accuracy: 0.7888

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7439

Epoch 2/3

Validation Accuracy: 0.7867

Epoch 3/3

Validation Accuracy: 0.7878

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7510

Epoch 2/3

Validation Accuracy: 0.7929

Epoch 3/3

Validation Accuracy: 0.7918

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7694

Epoch 2/4

Validation Accuracy: 0.7786

Epoch 3/4

Validation Accuracy: 0.7908

Epoch 4/4

Validation Accuracy: 0.7980

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7673

Epoch 2/4

Validation Accuracy: 0.7735

Epoch 3/4

Validation Accuracy: 0.7939

Epoch 4/4

Validation Accuracy: 0.8031

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7776

Epoch 2/4

Validation Accuracy: 0.7827

Epoch 3/4

Validation Accuracy: 0.7929

Epoch 4/4

Validation Accuracy: 0.7990

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7510

Epoch 2/4
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Validation Accuracy: 0.7908

Epoch 3/4

Validation Accuracy: 0.7959

Epoch 4/4

Validation Accuracy: 0.7939

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7500

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7663

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7531

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7459

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7633

Epoch 2/2

Validation Accuracy: 0.7857

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7571

Epoch 2/2

Validation Accuracy: 0.7776

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7541

Epoch 2/2

Validation Accuracy: 0.7786

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7653

Epoch 2/2

Validation Accuracy: 0.7837

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7633

Epoch 2/3

Validation Accuracy: 0.7816

Epoch 3/3

Validation Accuracy: 0.7857

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7653

Epoch 2/3

Validation Accuracy: 0.7806

Epoch 3/3

Validation Accuracy: 0.7857

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7704

Epoch 2/3

Validation Accuracy: 0.7878

Epoch 3/3

Validation Accuracy: 0.7939

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7327

Epoch 2/3

Validation Accuracy: 0.7837

Epoch 3/3

Validation Accuracy: 0.7867

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7531

Epoch 2/4

Validation Accuracy: 0.7796

Epoch 3/4

Validation Accuracy: 0.7929

Epoch 4/4

Validation Accuracy: 0.7939

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7531

Epoch 2/4

Validation Accuracy: 0.7888

Epoch 3/4

Validation Accuracy: 0.7939

Epoch 4/4

Validation Accuracy: 0.7939

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7684

Epoch 2/4

Validation Accuracy: 0.7857

Epoch 3/4

Validation Accuracy: 0.7857

Epoch 4/4

Validation Accuracy: 0.7857

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/4

Validation Accuracy: 0.7663

Epoch 2/4

Validation Accuracy: 0.7765

Epoch 3/4

Validation Accuracy: 0.7867

Epoch 4/4

Validation Accuracy: 0.7939

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7724

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7755

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7571

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7786

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7551

Epoch 2/2

Validation Accuracy: 0.7857

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7704

Epoch 2/2

Validation Accuracy: 0.7837

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7765

Epoch 2/2

Validation Accuracy: 0.7857

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7663

Epoch 2/2

Validation Accuracy: 0.7837

Best accuracy=0.8030612244897959
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7612

Epoch 2/3

Validation Accuracy: 0.7908

Epoch 3/3

Validation Accuracy: 0.8041

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7745

Epoch 2/3

Validation Accuracy: 0.7765

Epoch 3/3

Validation Accuracy: 0.7888

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7735

Epoch 2/3

Validation Accuracy: 0.7796

Epoch 3/3

Validation Accuracy: 0.7929

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7602

Epoch 2/3

Validation Accuracy: 0.7786

Epoch 3/3

Validation Accuracy: 0.7867

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7582

Epoch 2/4

Validation Accuracy: 0.7908

Epoch 3/4

Validation Accuracy: 0.7990

Epoch 4/4

Validation Accuracy: 0.7888

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7602

Epoch 2/4

Validation Accuracy: 0.7908

Epoch 3/4

Validation Accuracy: 0.7918

Epoch 4/4

Validation Accuracy: 0.7939

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7643

Epoch 2/4

Validation Accuracy: 0.7837

Epoch 3/4

Validation Accuracy: 0.7847

Epoch 4/4

Validation Accuracy: 0.7929

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7561

Epoch 2/4

Validation Accuracy: 0.8000

Epoch 3/4

Validation Accuracy: 0.7888

Epoch 4/4

Validation Accuracy: 0.7827

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7663

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7490

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7684

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7561

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7684

Epoch 2/2

Validation Accuracy: 0.7898

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7582

Epoch 2/2

Validation Accuracy: 0.7816

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7673

Epoch 2/2

Validation Accuracy: 0.7816

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7755

Epoch 2/2

Validation Accuracy: 0.7786

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7582

Epoch 2/3

Validation Accuracy: 0.7796

Epoch 3/3

Validation Accuracy: 0.7867

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7561

Epoch 2/3

Validation Accuracy: 0.7878

Epoch 3/3

Validation Accuracy: 0.7918

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7714

Epoch 2/3

Validation Accuracy: 0.7765

Epoch 3/3

Validation Accuracy: 0.7816

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7673

Epoch 2/3

Validation Accuracy: 0.7969

Epoch 3/3

Validation Accuracy: 0.7918

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7735

Epoch 2/4

Validation Accuracy: 0.7929

Epoch 3/4

Validation Accuracy: 0.7929

Epoch 4/4

Validation Accuracy: 0.7939

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7520

Epoch 2/4

Validation Accuracy: 0.7857

Epoch 3/4

Validation Accuracy: 0.7898

Epoch 4/4

Validation Accuracy: 0.7939

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7735

Epoch 2/4

Validation Accuracy: 0.7724

Epoch 3/4

Validation Accuracy: 0.7857

Epoch 4/4

Validation Accuracy: 0.7969

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7500

Epoch 2/4

Validation Accuracy: 0.7582

Epoch 3/4

Validation Accuracy: 0.7867

Epoch 4/4

Validation Accuracy: 0.7918

Best accuracy=0.8040816326530612
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
Saving best performing model...
Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 180, in <module>
    torch.save(best_model_state, Path(f"src/classifier/models/bert_{bert_model_name}_{topic}_classifier.pth"))
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 716, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 687, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory src/classifier/models/bert_GroNLP does not exist.
Running train_bert_classfier.py with arguments GroNLP/bert-base-dutch-cased and Immigration
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.8028

Best accuracy=0.8027867095391211
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7953

Best accuracy=0.8027867095391211
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7899

Best accuracy=0.8027867095391211
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7964

Best accuracy=0.8027867095391211
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7867

Epoch 2/2

Validation Accuracy: 0.8178

Best accuracy=0.8177920685959271
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.8178

Epoch 2/2

Validation Accuracy: 0.8253

Best accuracy=0.8252947481243301
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7910

Epoch 2/2

Validation Accuracy: 0.8274

Best accuracy=0.827438370846731
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7942

Epoch 2/2

Validation Accuracy: 0.8167

Best accuracy=0.827438370846731
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.8124

Epoch 2/3

Validation Accuracy: 0.8285

Epoch 3/3

Validation Accuracy: 0.8349

Best accuracy=0.834941050375134
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.8028

Epoch 2/3

Validation Accuracy: 0.8307

Epoch 3/3

Validation Accuracy: 0.8360

Best accuracy=0.8360128617363344
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7985

Epoch 2/3

Validation Accuracy: 0.8274

Epoch 3/3

Validation Accuracy: 0.8274

Best accuracy=0.8360128617363344
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.8060

Epoch 2/3

Validation Accuracy: 0.8242

Epoch 3/3

Validation Accuracy: 0.8307

Best accuracy=0.8360128617363344
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.8114

Epoch 2/4

Validation Accuracy: 0.8221

Epoch 3/4

Validation Accuracy: 0.8242

Epoch 4/4

Validation Accuracy: 0.8232

Best accuracy=0.8360128617363344
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.8060

Epoch 2/4

Validation Accuracy: 0.8232

Epoch 3/4

Validation Accuracy: 0.8424

Epoch 4/4

Validation Accuracy: 0.8349

Best accuracy=0.8360128617363344
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8039

Epoch 2/4

Validation Accuracy: 0.8339

Epoch 3/4

Validation Accuracy: 0.8457

Epoch 4/4

Validation Accuracy: 0.8349

Best accuracy=0.8360128617363344
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.8103

Epoch 2/4

Validation Accuracy: 0.8285

Epoch 3/4

Validation Accuracy: 0.8392

Epoch 4/4

Validation Accuracy: 0.8414

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7599

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7631

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7653

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7803

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7964

Epoch 2/2

Validation Accuracy: 0.8124

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7856

Epoch 2/2

Validation Accuracy: 0.8135

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7953

Epoch 2/2

Validation Accuracy: 0.8135

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7728

Epoch 2/2

Validation Accuracy: 0.8071

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7867

Epoch 2/3

Validation Accuracy: 0.8253

Epoch 3/3

Validation Accuracy: 0.8296

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7899

Epoch 2/3

Validation Accuracy: 0.8296

Epoch 3/3

Validation Accuracy: 0.8339

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.8060

Epoch 2/3

Validation Accuracy: 0.8210

Epoch 3/3

Validation Accuracy: 0.8339

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7942

Epoch 2/3

Validation Accuracy: 0.8210

Epoch 3/3

Validation Accuracy: 0.8307

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7899

Epoch 2/4

Validation Accuracy: 0.8210

Epoch 3/4

Validation Accuracy: 0.8285

Epoch 4/4

Validation Accuracy: 0.8264

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7953

Epoch 2/4

Validation Accuracy: 0.8242

Epoch 3/4

Validation Accuracy: 0.8328

Epoch 4/4

Validation Accuracy: 0.8328

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8017

Epoch 2/4

Validation Accuracy: 0.8296

Epoch 3/4

Validation Accuracy: 0.8317

Epoch 4/4

Validation Accuracy: 0.8328

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.8103

Epoch 2/4

Validation Accuracy: 0.8360

Epoch 3/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.8349

Epoch 4/4

Validation Accuracy: 0.8328

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.8060

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.8028

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.8060

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7974

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.8092

Epoch 2/2

Validation Accuracy: 0.8349

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.8039

Epoch 2/2

Validation Accuracy: 0.8285

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7824

Epoch 2/2

Validation Accuracy: 0.8307

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7921

Epoch 2/2

Validation Accuracy: 0.8253

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.8071

Epoch 2/3

Validation Accuracy: 0.8232

Epoch 3/3

Validation Accuracy: 0.8349

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.8135

Epoch 2/3

Validation Accuracy: 0.8328

Epoch 3/3

Validation Accuracy: 0.8360

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7953

Epoch 2/3

Validation Accuracy: 0.8339

Epoch 3/3

Validation Accuracy: 0.8403

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.8039

Epoch 2/3

Validation Accuracy: 0.8221

Epoch 3/3

Validation Accuracy: 0.8296

Best accuracy=0.8413719185423365
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.8167

Epoch 2/4

Validation Accuracy: 0.8457

Epoch 3/4

Validation Accuracy: 0.8446

Epoch 4/4

Validation Accuracy: 0.8457

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.8210

Epoch 2/4

Validation Accuracy: 0.8307

Epoch 3/4

Validation Accuracy: 0.8382

Epoch 4/4

Validation Accuracy: 0.8382

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8006

Epoch 2/4

Validation Accuracy: 0.8339

Epoch 3/4

Validation Accuracy: 0.8274

Epoch 4/4

Validation Accuracy: 0.8307

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.8296

Epoch 2/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.8328

Epoch 3/4

Validation Accuracy: 0.8371

Epoch 4/4

Validation Accuracy: 0.8446

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7974

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7921

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7942

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7663

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.8039

Epoch 2/2

Validation Accuracy: 0.8232

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.8135

Epoch 2/2

Validation Accuracy: 0.8242

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.8156

Epoch 2/2

Validation Accuracy: 0.8328

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.8103

Epoch 2/2

Validation Accuracy: 0.8296

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7942

Epoch 2/3

Validation Accuracy: 0.8189

Epoch 3/3

Validation Accuracy: 0.8199

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7889

Epoch 2/3

Validation Accuracy: 0.8178

Epoch 3/3

Validation Accuracy: 0.8296

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7985

Epoch 2/3

Validation Accuracy: 0.8349

Epoch 3/3

Validation Accuracy: 0.8307

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.8039

Epoch 2/3

Validation Accuracy: 0.8328

Epoch 3/3

Validation Accuracy: 0.8371

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.8092

Epoch 2/4

Validation Accuracy: 0.8199

Epoch 3/4

Validation Accuracy: 0.8317

Epoch 4/4

Validation Accuracy: 0.8371

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7974

Epoch 2/4

Validation Accuracy: 0.8274

Epoch 3/4

Validation Accuracy: 0.8371

Epoch 4/4

Validation Accuracy: 0.8360

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8114

Epoch 2/4

Validation Accuracy: 0.8349

Epoch 3/4

Validation Accuracy: 0.8328

Epoch 4/4

Validation Accuracy: 0.8403

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/4

Validation Accuracy: 0.8028

Epoch 2/4

Validation Accuracy: 0.8210

Epoch 3/4

Validation Accuracy: 0.8403

Epoch 4/4

Validation Accuracy: 0.8360

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.8221

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.8167

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7985

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.8167

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.8092

Epoch 2/2

Validation Accuracy: 0.8221

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.8006

Epoch 2/2

Validation Accuracy: 0.8264

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.8092

Epoch 2/2

Validation Accuracy: 0.8296

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.8135

Epoch 2/2

Validation Accuracy: 0.8189

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7942

Epoch 2/3

Validation Accuracy: 0.8360

Epoch 3/3

Validation Accuracy: 0.8382

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7814

Epoch 2/3

Validation Accuracy: 0.8296

Epoch 3/3

Validation Accuracy: 0.8360

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.8060

Epoch 2/3

Validation Accuracy: 0.8371

Epoch 3/3

Validation Accuracy: 0.8414

Best accuracy=0.8456591639871383
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.8156

Epoch 2/3

Validation Accuracy: 0.8414

Epoch 3/3

Validation Accuracy: 0.8467

Best accuracy=0.8467309753483387
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.8081

Epoch 2/4

Validation Accuracy: 0.8146

Epoch 3/4

Validation Accuracy: 0.8296

Epoch 4/4

Validation Accuracy: 0.8328

Best accuracy=0.8467309753483387
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7974

Epoch 2/4

Validation Accuracy: 0.8360

Epoch 3/4

Validation Accuracy: 0.8467

Epoch 4/4

Validation Accuracy: 0.8467

Best accuracy=0.8467309753483387
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8039

Epoch 2/4

Validation Accuracy: 0.8274

Epoch 3/4

Validation Accuracy: 0.8349

Epoch 4/4

Validation Accuracy: 0.8478

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.8028

Epoch 2/4

Validation Accuracy: 0.8382

Epoch 3/4

Validation Accuracy: 0.8435

Epoch 4/4

Validation Accuracy: 0.8457

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7974

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.8092

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7996

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7942

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.8049

Epoch 2/2

Validation Accuracy: 0.8274

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.8006

Epoch 2/2

Validation Accuracy: 0.8328

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.8114

Epoch 2/2

Validation Accuracy: 0.8264

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.8028

Epoch 2/2

Validation Accuracy: 0.8296

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.8060

Epoch 2/3

Validation Accuracy: 0.8296

Epoch 3/3

Validation Accuracy: 0.8360

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.8135

Epoch 2/3

Validation Accuracy: 0.8339

Epoch 3/3

Validation Accuracy: 0.8403

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7942

Epoch 2/3

Validation Accuracy: 0.8296

Epoch 3/3

Validation Accuracy: 0.8382

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.8114

Epoch 2/3

Validation Accuracy: 0.8242

Epoch 3/3

Validation Accuracy: 0.8264

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.8081

Epoch 2/4

Validation Accuracy: 0.8285

Epoch 3/4

Validation Accuracy: 0.8339

Epoch 4/4

Validation Accuracy: 0.8382

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.8092

Epoch 2/4

Validation Accuracy: 0.8317

Epoch 3/4

Validation Accuracy: 0.8339

Epoch 4/4

Validation Accuracy: 0.8339

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7953

Epoch 2/4

Validation Accuracy: 0.8403

Epoch 3/4

Validation Accuracy: 0.8446

Epoch 4/4

Validation Accuracy: 0.8478

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.8092

Epoch 2/4

Validation Accuracy: 0.8307

Epoch 3/4

Validation Accuracy: 0.8392

Epoch 4/4

Validation Accuracy: 0.8478

Best accuracy=0.8478027867095391
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
Saving best performing model...
Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 180, in <module>
    torch.save(best_model_state, Path(f"src/classifier/models/bert_{bert_model_name}_{topic}_classifier.pth"))
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 716, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 687, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory src/classifier/models/bert_GroNLP does not exist.
Running train_bert_classfier.py with arguments GroNLP/bert-base-dutch-cased and Economic
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7675

Best accuracy=0.7675070028011205
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7731

Best accuracy=0.773109243697479
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7661

Best accuracy=0.773109243697479
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7465

Best accuracy=0.773109243697479
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7759

Epoch 2/2

Validation Accuracy: 0.8053

Best accuracy=0.8053221288515406
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7773

Epoch 2/2

Validation Accuracy: 0.8067

Best accuracy=0.8067226890756303
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7871

Epoch 2/2

Validation Accuracy: 0.8165

Best accuracy=0.8165266106442577
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7689

Epoch 2/2

Validation Accuracy: 0.8067

Best accuracy=0.8165266106442577
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7955

Epoch 2/3

Validation Accuracy: 0.8193

Epoch 3/3

Validation Accuracy: 0.8221

Best accuracy=0.8221288515406162
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7717

Epoch 2/3

Validation Accuracy: 0.8207

Epoch 3/3

Validation Accuracy: 0.8235

Best accuracy=0.8235294117647058
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7913

Epoch 2/3

Validation Accuracy: 0.8123

Epoch 3/3

Validation Accuracy: 0.8137

Best accuracy=0.8235294117647058
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7983

Epoch 2/3

Validation Accuracy: 0.8067

Epoch 3/3

Validation Accuracy: 0.8151

Best accuracy=0.8235294117647058
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.8123

Epoch 2/4

Validation Accuracy: 0.8249

Epoch 3/4

Validation Accuracy: 0.8179

Epoch 4/4

Validation Accuracy: 0.8249

Best accuracy=0.8249299719887955
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7997

Epoch 2/4

Validation Accuracy: 0.8207

Epoch 3/4

Validation Accuracy: 0.8151

Epoch 4/4

Validation Accuracy: 0.8263

Best accuracy=0.8263305322128851
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8151

Epoch 2/4

Validation Accuracy: 0.8221

Epoch 3/4

Validation Accuracy: 0.8207

Epoch 4/4

Validation Accuracy: 0.8277

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7983

Epoch 2/4

Validation Accuracy: 0.8011

Epoch 3/4

Validation Accuracy: 0.8123

Epoch 4/4

Validation Accuracy: 0.8193

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7479

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7409

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7409

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7451

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7661

Epoch 2/2

Validation Accuracy: 0.7913

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7661

Epoch 2/2

Validation Accuracy: 0.7745

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7689

Epoch 2/2

Validation Accuracy: 0.7843

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7633

Epoch 2/2

Validation Accuracy: 0.7759

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7661

Epoch 2/3

Validation Accuracy: 0.7913

Epoch 3/3

Validation Accuracy: 0.8053

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7885

Epoch 2/3

Validation Accuracy: 0.8025

Epoch 3/3

Validation Accuracy: 0.8025

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7773

Epoch 2/3

Validation Accuracy: 0.8109

Epoch 3/3

Validation Accuracy: 0.8207

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7843

Epoch 2/3

Validation Accuracy: 0.8053

Epoch 3/3

Validation Accuracy: 0.8137

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7619

Epoch 2/4

Validation Accuracy: 0.8081

Epoch 3/4

Validation Accuracy: 0.8221

Epoch 4/4

Validation Accuracy: 0.8165

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7703

Epoch 2/4

Validation Accuracy: 0.8025

Epoch 3/4

Validation Accuracy: 0.8221

Epoch 4/4

Validation Accuracy: 0.8165

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7633

Epoch 2/4

Validation Accuracy: 0.8221

Epoch 3/4

Validation Accuracy: 0.8011

Epoch 4/4

Validation Accuracy: 0.8123

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7647

Epoch 2/4

Validation Accuracy: 0.7941

Epoch 3/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.8165

Epoch 4/4

Validation Accuracy: 0.8221

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7815

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7969

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7745

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7801

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7941

Epoch 2/2

Validation Accuracy: 0.8137

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7773

Epoch 2/2

Validation Accuracy: 0.8123

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7941

Epoch 2/2

Validation Accuracy: 0.8137

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7759

Epoch 2/2

Validation Accuracy: 0.8039

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.8067

Epoch 2/3

Validation Accuracy: 0.8193

Epoch 3/3

Validation Accuracy: 0.8221

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7941

Epoch 2/3

Validation Accuracy: 0.8207

Epoch 3/3

Validation Accuracy: 0.8207

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7899

Epoch 2/3

Validation Accuracy: 0.8193

Epoch 3/3

Validation Accuracy: 0.8263

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7941

Epoch 2/3

Validation Accuracy: 0.8081

Epoch 3/3

Validation Accuracy: 0.8193

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7983

Epoch 2/4

Validation Accuracy: 0.8333

Epoch 3/4

Validation Accuracy: 0.8333

Epoch 4/4

Validation Accuracy: 0.8277

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7955

Epoch 2/4

Validation Accuracy: 0.8207

Epoch 3/4

Validation Accuracy: 0.8235

Epoch 4/4

Validation Accuracy: 0.8249

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8039

Epoch 2/4

Validation Accuracy: 0.8123

Epoch 3/4

Validation Accuracy: 0.8179

Epoch 4/4

Validation Accuracy: 0.8249

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7843

Epoch 2/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.8137

Epoch 3/4

Validation Accuracy: 0.8095

Epoch 4/4

Validation Accuracy: 0.8235

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7717

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7773

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7689

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7815

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7843

Epoch 2/2

Validation Accuracy: 0.8039

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7773

Epoch 2/2

Validation Accuracy: 0.7969

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7899

Epoch 2/2

Validation Accuracy: 0.7969

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7745

Epoch 2/2

Validation Accuracy: 0.8151

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.8039

Epoch 2/3

Validation Accuracy: 0.8053

Epoch 3/3

Validation Accuracy: 0.7983

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7983

Epoch 2/3

Validation Accuracy: 0.8361

Epoch 3/3

Validation Accuracy: 0.8263

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7857

Epoch 2/3

Validation Accuracy: 0.8053

Epoch 3/3

Validation Accuracy: 0.8235

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7871

Epoch 2/3

Validation Accuracy: 0.8039

Epoch 3/3

Validation Accuracy: 0.8095

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7983

Epoch 2/4

Validation Accuracy: 0.8165

Epoch 3/4

Validation Accuracy: 0.8221

Epoch 4/4

Validation Accuracy: 0.8193

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7703

Epoch 2/4

Validation Accuracy: 0.8137

Epoch 3/4

Validation Accuracy: 0.8165

Epoch 4/4

Validation Accuracy: 0.8165

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7941

Epoch 2/4

Validation Accuracy: 0.8123

Epoch 3/4

Validation Accuracy: 0.8137

Epoch 4/4

Validation Accuracy: 0.8137

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/4

Validation Accuracy: 0.7927

Epoch 2/4

Validation Accuracy: 0.8095

Epoch 3/4

Validation Accuracy: 0.8151

Epoch 4/4

Validation Accuracy: 0.8221

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7913

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.8025

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.8151

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7927

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.8011

Epoch 2/2

Validation Accuracy: 0.8151

Best accuracy=0.8277310924369747
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=4
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7829

Epoch 2/2

Validation Accuracy: 0.8291

Best accuracy=0.8291316526610645
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7983

Epoch 2/2

Validation Accuracy: 0.8235

Best accuracy=0.8291316526610645
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7955

Epoch 2/2

Validation Accuracy: 0.8137

Best accuracy=0.8291316526610645
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.8095

Epoch 2/3

Validation Accuracy: 0.8235

Epoch 3/3

Validation Accuracy: 0.8333

Best accuracy=0.8333333333333334
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.8081

Epoch 2/3

Validation Accuracy: 0.8277

Epoch 3/3

Validation Accuracy: 0.8347

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7899

Epoch 2/3

Validation Accuracy: 0.8137

Epoch 3/3

Validation Accuracy: 0.8263

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.8081

Epoch 2/3

Validation Accuracy: 0.8095

Epoch 3/3

Validation Accuracy: 0.8207

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.8025

Epoch 2/4

Validation Accuracy: 0.8361

Epoch 3/4

Validation Accuracy: 0.8333

Epoch 4/4

Validation Accuracy: 0.8249

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.8011

Epoch 2/4

Validation Accuracy: 0.8291

Epoch 3/4

Validation Accuracy: 0.8319

Epoch 4/4

Validation Accuracy: 0.8305

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.8011

Epoch 2/4

Validation Accuracy: 0.8235

Epoch 3/4

Validation Accuracy: 0.8249

Epoch 4/4

Validation Accuracy: 0.8277

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7941

Epoch 2/4

Validation Accuracy: 0.8137

Epoch 3/4

Validation Accuracy: 0.8305

Epoch 4/4

Validation Accuracy: 0.8319

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7745

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7927

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7759

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7773

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7871

Epoch 2/2

Validation Accuracy: 0.8067

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7941

Epoch 2/2

Validation Accuracy: 0.8109

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7773

Epoch 2/2

Validation Accuracy: 0.8123

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7927

Epoch 2/2

Validation Accuracy: 0.8067

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.8081

Epoch 2/3

Validation Accuracy: 0.8137

Epoch 3/3

Validation Accuracy: 0.8221

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.8053

Epoch 2/3

Validation Accuracy: 0.8123

Epoch 3/3

Validation Accuracy: 0.8207

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7955

Epoch 2/3

Validation Accuracy: 0.8137

Epoch 3/3

Validation Accuracy: 0.8165

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7969

Epoch 2/3

Validation Accuracy: 0.8025

Epoch 3/3

Validation Accuracy: 0.8109

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7857

Epoch 2/4

Validation Accuracy: 0.8109

Epoch 3/4

Validation Accuracy: 0.8277

Epoch 4/4

Validation Accuracy: 0.8333

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.8053

Epoch 2/4

Validation Accuracy: 0.8235

Epoch 3/4

Validation Accuracy: 0.8263

Epoch 4/4

Validation Accuracy: 0.8347

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7955

Epoch 2/4

Validation Accuracy: 0.8095

Epoch 3/4

Validation Accuracy: 0.8179

Epoch 4/4

Validation Accuracy: 0.8221

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.8067

Epoch 2/4

Validation Accuracy: 0.8235

Epoch 3/4

Validation Accuracy: 0.8193

Epoch 4/4

Validation Accuracy: 0.8291

Best accuracy=0.834733893557423
                Best Hyperparameters: 
                Learning Rate=5e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
Saving best performing model...
Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 180, in <module>
    torch.save(best_model_state, Path(f"src/classifier/models/bert_{bert_model_name}_{topic}_classifier.pth"))
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 716, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 687, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory src/classifier/models/bert_GroNLP does not exist.
Running train_bert_classfier.py with arguments GroNLP/bert-base-dutch-cased and Agriculture
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7583

Best accuracy=0.7583333333333333
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7667

Best accuracy=0.7666666666666667
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7611

Best accuracy=0.7666666666666667
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7417

Best accuracy=0.7666666666666667
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7750

Epoch 2/2

Validation Accuracy: 0.7750

Best accuracy=0.775
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7694

Epoch 2/2

Validation Accuracy: 0.7778

Best accuracy=0.7777777777777778
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7667

Epoch 2/2

Validation Accuracy: 0.7833

Best accuracy=0.7833333333333333
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7472

Epoch 2/2

Validation Accuracy: 0.7778

Best accuracy=0.7833333333333333
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.6861

Epoch 2/3

Validation Accuracy: 0.7583

Epoch 3/3

Validation Accuracy: 0.7778

Best accuracy=0.7833333333333333
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7639

Epoch 2/3

Validation Accuracy: 0.7611

Epoch 3/3

Validation Accuracy: 0.7694

Best accuracy=0.7833333333333333
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=2
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7667

Epoch 2/3

Validation Accuracy: 0.7861

Epoch 3/3

Validation Accuracy: 0.7944

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7361

Epoch 2/3

Validation Accuracy: 0.7778

Epoch 3/3

Validation Accuracy: 0.7806

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7500

Epoch 2/4

Validation Accuracy: 0.7611

Epoch 3/4

Validation Accuracy: 0.7889

Epoch 4/4

Validation Accuracy: 0.7778

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7500

Epoch 2/4

Validation Accuracy: 0.7694

Epoch 3/4

Validation Accuracy: 0.7694

Epoch 4/4

Validation Accuracy: 0.7806

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7750

Epoch 2/4

Validation Accuracy: 0.7917

Epoch 3/4

Validation Accuracy: 0.7778

Epoch 4/4

Validation Accuracy: 0.7944

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7528

Epoch 2/4

Validation Accuracy: 0.7806

Epoch 3/4

Validation Accuracy: 0.7806

Epoch 4/4

Validation Accuracy: 0.7722

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7028

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7306

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7417

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7333

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7306

Epoch 2/2

Validation Accuracy: 0.7639

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7639

Epoch 2/2

Validation Accuracy: 0.7889

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7417

Epoch 2/2

Validation Accuracy: 0.7694

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7528

Epoch 2/2

Validation Accuracy: 0.7472

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7528

Epoch 2/3

Validation Accuracy: 0.7833

Epoch 3/3

Validation Accuracy: 0.7639

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7500

Epoch 2/3

Validation Accuracy: 0.7639

Epoch 3/3

Validation Accuracy: 0.7667

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7417

Epoch 2/3

Validation Accuracy: 0.7583

Epoch 3/3

Validation Accuracy: 0.7639

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7667

Epoch 2/3

Validation Accuracy: 0.7722

Epoch 3/3

Validation Accuracy: 0.7778

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7583

Epoch 2/4

Validation Accuracy: 0.7694

Epoch 3/4

Validation Accuracy: 0.7806

Epoch 4/4

Validation Accuracy: 0.7833

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7389

Epoch 2/4

Validation Accuracy: 0.7639

Epoch 3/4

Validation Accuracy: 0.7722

Epoch 4/4

Validation Accuracy: 0.7667

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7611

Epoch 2/4

Validation Accuracy: 0.7611

Epoch 3/4

Validation Accuracy: 0.7806

Epoch 4/4

Validation Accuracy: 0.7778

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7667

Epoch 2/4

Validation Accuracy: 0.7694

Epoch 3/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.7694

Epoch 4/4

Validation Accuracy: 0.7750

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7806

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7528

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7528

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7639

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7667

Epoch 2/2

Validation Accuracy: 0.7667

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7500

Epoch 2/2

Validation Accuracy: 0.7667

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7778

Epoch 2/2

Validation Accuracy: 0.7667

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7528

Epoch 2/2

Validation Accuracy: 0.7806

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7444

Epoch 2/3

Validation Accuracy: 0.7750

Epoch 3/3

Validation Accuracy: 0.7722

Best accuracy=0.7944444444444444
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.3
                Max Length=128

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7778

Epoch 2/3

Validation Accuracy: 0.7667

Epoch 3/3

Validation Accuracy: 0.8000

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7583

Epoch 2/3

Validation Accuracy: 0.7806

Epoch 3/3

Validation Accuracy: 0.7861

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7528

Epoch 2/3

Validation Accuracy: 0.7778

Epoch 3/3

Validation Accuracy: 0.7806

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7667

Epoch 2/4

Validation Accuracy: 0.7639

Epoch 3/4

Validation Accuracy: 0.7861

Epoch 4/4

Validation Accuracy: 0.7833

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7361

Epoch 2/4

Validation Accuracy: 0.7944

Epoch 3/4

Validation Accuracy: 0.7778

Epoch 4/4

Validation Accuracy: 0.7833

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7472

Epoch 2/4

Validation Accuracy: 0.7556

Epoch 3/4

Validation Accuracy: 0.7833

Epoch 4/4

Validation Accuracy: 0.7833

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7417

Epoch 2/4

Validation Accuracy: 0.7778

Epoch 3/4

Validation Accuracy: 0.7806

Epoch 4/4

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Validation Accuracy: 0.7917

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7417

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7556

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7722

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7500

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7500

Epoch 2/2

Validation Accuracy: 0.7667

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7778

Epoch 2/2

Validation Accuracy: 0.7667

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7694

Epoch 2/2

Validation Accuracy: 0.7889

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7417

Epoch 2/2

Validation Accuracy: 0.7778

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7667

Epoch 2/3

Validation Accuracy: 0.7528

Epoch 3/3

Validation Accuracy: 0.7639

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7472

Epoch 2/3

Validation Accuracy: 0.7694

Epoch 3/3

Validation Accuracy: 0.7778

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7611

Epoch 2/3

Validation Accuracy: 0.7639

Epoch 3/3

Validation Accuracy: 0.7750

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7417

Epoch 2/3

Validation Accuracy: 0.7472

Epoch 3/3

Validation Accuracy: 0.7639

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7583

Epoch 2/4

Validation Accuracy: 0.7806

Epoch 3/4

Validation Accuracy: 0.7778

Epoch 4/4

Validation Accuracy: 0.7833

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7694

Epoch 2/4

Validation Accuracy: 0.7778

Epoch 3/4

Validation Accuracy: 0.7806

Epoch 4/4

Validation Accuracy: 0.7833

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7722

Epoch 2/4

Validation Accuracy: 0.7556

Epoch 3/4

Validation Accuracy: 0.7917

Epoch 4/4

Validation Accuracy: 0.7861

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=3e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7639

Epoch 2/4

Validation Accuracy: 0.7750

Epoch 3/4

Validation Accuracy: 0.7639

Epoch 4/4

Validation Accuracy: 0.7861

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7583

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7528

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7389

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7778

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7972

Epoch 2/2

Validation Accuracy: 0.7778

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7833

Epoch 2/2

Validation Accuracy: 0.7833

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7444

Epoch 2/2

Validation Accuracy: 0.7917

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7667

Epoch 2/2

Validation Accuracy: 0.7861

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7528

Epoch 2/3

Validation Accuracy: 0.7778

Epoch 3/3

Validation Accuracy: 0.7861

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7417

Epoch 2/3

Validation Accuracy: 0.7694

Epoch 3/3

Validation Accuracy: 0.7750

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7611

Epoch 2/3

Validation Accuracy: 0.7722

Epoch 3/3

Validation Accuracy: 0.7889

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7333

Epoch 2/3

Validation Accuracy: 0.7778

Epoch 3/3

Validation Accuracy: 0.7806

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7917

Epoch 2/4

Validation Accuracy: 0.7750

Epoch 3/4

Validation Accuracy: 0.7806

Epoch 4/4

Validation Accuracy: 0.7889

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7639

Epoch 2/4

Validation Accuracy: 0.7861

Epoch 3/4

Validation Accuracy: 0.7889

Epoch 4/4

Validation Accuracy: 0.7889

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7417

Epoch 2/4

Validation Accuracy: 0.7667

Epoch 3/4

Validation Accuracy: 0.7778

Epoch 4/4

Validation Accuracy: 0.7889

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=8, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7389

Epoch 2/4

Validation Accuracy: 0.7667

Epoch 3/4

Validation Accuracy: 0.7972

Epoch 4/4

Validation Accuracy: 0.7889

Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertModel were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7611

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Validation Accuracy: 0.7667

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=128

Epoch 1/1

Validation Accuracy: 0.7583

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=1, dropout_rate=0.3, max_length=256

Epoch 1/1

Validation Accuracy: 0.7611

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=128

Epoch 1/2

Validation Accuracy: 0.7556

Epoch 2/2

Validation Accuracy: 0.7806

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.1, max_length=256

Epoch 1/2

Validation Accuracy: 0.7417

Epoch 2/2

Validation Accuracy: 0.7667

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=128

Epoch 1/2

Validation Accuracy: 0.7639

Epoch 2/2

Validation Accuracy: 0.7667

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=2, dropout_rate=0.3, max_length=256

Epoch 1/2

Validation Accuracy: 0.7889

Epoch 2/2

Validation Accuracy: 0.7944

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=128

Epoch 1/3

Validation Accuracy: 0.7667

Epoch 2/3

Validation Accuracy: 0.7667

Epoch 3/3

Validation Accuracy: 0.7722

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.1, max_length=256

Epoch 1/3

Validation Accuracy: 0.7583

Epoch 2/3

Validation Accuracy: 0.7583

Epoch 3/3

Validation Accuracy: 0.7722

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=128

Epoch 1/3

Validation Accuracy: 0.7278

Epoch 2/3

Validation Accuracy: 0.7778

Epoch 3/3

Validation Accuracy: 0.7778

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=3, dropout_rate=0.3, max_length=256

Epoch 1/3

Validation Accuracy: 0.7667

Epoch 2/3

Validation Accuracy: 0.7861

Epoch 3/3

Validation Accuracy: 0.7750

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=128

Epoch 1/4

Validation Accuracy: 0.7833

Epoch 2/4

Validation Accuracy: 0.7806

Epoch 3/4

Validation Accuracy: 0.7833

Epoch 4/4

Validation Accuracy: 0.7917

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.1, max_length=256

Epoch 1/4

Validation Accuracy: 0.7694

Epoch 2/4

Validation Accuracy: 0.7583

Epoch 3/4

Validation Accuracy: 0.7889

Epoch 4/4

Validation Accuracy: 0.7861

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=128

Epoch 1/4

Validation Accuracy: 0.7556

Epoch 2/4

Validation Accuracy: 0.7583

Epoch 3/4

Validation Accuracy: 0.7750

Epoch 4/4

Validation Accuracy: 0.7889

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
______________________________________________________________________________
Training with lr=5e-05, batch_size=16, num_epochs=4, dropout_rate=0.3, max_length=256

Epoch 1/4

Validation Accuracy: 0.7806

Epoch 2/4

Validation Accuracy: 0.7639

Epoch 3/4

Validation Accuracy: 0.7917

Epoch 4/4

Validation Accuracy: 0.7889

Best accuracy=0.8
                Best Hyperparameters: 
                Learning Rate=3e-05 
                Batch Size=8
                Num Epochs=3
                Dropout Rate=0.1
                Max Length=256

                
Saving best performing model...
Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 180, in <module>
    torch.save(best_model_state, Path(f"src/classifier/models/bert_{bert_model_name}_{topic}_classifier.pth"))
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 849, in save
    with _open_zipfile_writer(f) as opened_zipfile:
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 716, in _open_zipfile_writer
    return container(name_or_buffer)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/serialization.py", line 687, in __init__
    super().__init__(torch._C.PyTorchFileWriter(self.name))
RuntimeError: Parent directory src/classifier/models/bert_GroNLP does not exist.
Running train_bert_classfier.py with arguments xlm-roberta-base and CivilRights
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.5887

Best accuracy=0.5887290167865707
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 147, in <module>
    train(model, train_dataloader, optimizer, scheduler, device)
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 58, in train
    optimizer.step()
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/transformers/optimization.py", line 648, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 512.06 MiB is free. Including non-PyTorch memory, this process has 6.81 GiB memory in use. Of the allocated memory 5.22 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running train_bert_classfier.py with arguments xlm-roberta-base and Environment
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.6490

Best accuracy=0.6489795918367347
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 147, in <module>
    train(model, train_dataloader, optimizer, scheduler, device)
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 58, in train
    optimizer.step()
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/transformers/optimization.py", line 648, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 546.06 MiB is free. Including non-PyTorch memory, this process has 6.78 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running train_bert_classfier.py with arguments xlm-roberta-base and Immigration
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.7181

Best accuracy=0.7181136120042872
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 147, in <module>
    train(model, train_dataloader, optimizer, scheduler, device)
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 58, in train
    optimizer.step()
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/transformers/optimization.py", line 648, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 536.06 MiB is free. Including non-PyTorch memory, this process has 6.79 GiB memory in use. Of the allocated memory 5.21 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running train_bert_classfier.py with arguments xlm-roberta-base and Economic
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.6092

Best accuracy=0.6092436974789915
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 147, in <module>
    train(model, train_dataloader, optimizer, scheduler, device)
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 58, in train
    optimizer.step()
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/transformers/optimization.py", line 648, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 512.06 MiB is free. Including non-PyTorch memory, this process has 6.81 GiB memory in use. Of the allocated memory 5.22 GiB is allocated by PyTorch, and 1.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Running train_bert_classfier.py with arguments xlm-roberta-base and Agriculture
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=128

Epoch 1/1

Validation Accuracy: 0.4861

Best accuracy=0.4861111111111111
                Best Hyperparameters: 
                Learning Rate=2e-05 
                Batch Size=8
                Num Epochs=1
                Dropout Rate=0.1
                Max Length=128

                
______________________________________________________________________________
Training with lr=2e-05, batch_size=8, num_epochs=1, dropout_rate=0.1, max_length=256

Epoch 1/1

Traceback (most recent call last):
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 147, in <module>
    train(model, train_dataloader, optimizer, scheduler, device)
  File "/home/jelle/Documents/issuepositions/src/classifier/train_bert_classifier.py", line 58, in train
    optimizer.step()
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/jelle/.local/lib/python3.10/site-packages/transformers/optimization.py", line 648, in step
    denom = exp_avg_sq.sqrt().add_(group["eps"])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 536.06 MiB is free. Including non-PyTorch memory, this process has 6.79 GiB memory in use. Of the allocated memory 5.22 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

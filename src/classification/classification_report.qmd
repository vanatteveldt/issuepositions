---
title: "Performance of automatic classification"
format: 
  gfm:
    fig-path: "classificationreport-figures/"
editor_options: 
  chunk_output_type: console
execute:
  echo: false
  message: false
  warning: false  
---

```{r data}
library(tidyverse)
library(yardstick)
data_root = here::here("data/intermediate/classification")

read_file <- function(fn) {
  fields = str_extract(fn, ".*/([^/]+)\\.csv", group = 1) |> str_split("_") |> first()
  read_csv(fn, show_col_types = FALSE, name_repair = "unique_quiet") |> 
    select(-`...1`, -Index) |> 
    add_column(fn=fn, model = fields[2], shot = fields[3], reason = fields[4], .before=1)
}

d <- list.files(data_root, full.names = T) |> map(read_file, .progress=T) |> list_rbind()
d <- d |> mutate(stance=as.factor(stance), 
                 prediction=if_else(prediction == "Neutral / No Stance", "N", prediction) |> as.factor())

n_sent <- d |> group_by(model, shot, reason, topic) |> summarize(n=n()) |> pull(n) |> mean()
n_topic <- length(unique(d$topic))
```


We tested a variety of models in different x-shot settings and with ~~or without~~ chain-of-reasoning prompts. 

Specifically, the model was asked to judge the stance of actors in `r n_sent` sentences on `r n_topic` topics.
The specific meaning of each stance depends on the topic, but the options were always L (mostly left/progressive stances), N (neutral) or R (more right-wing/conservative). For more information, see the [codebook](/codebook/codebook.md) and [topic list](/codebook/topics-en.md) ([dutch](/codebook/topics-nl.md)). 

# Overall performance

The table below gives the overall (macro-averaged) performance of each model:

```{r overall}
d |> group_by(model, shot, reason) |> 
  summarize(n=n(), acc=mean(stance==prediction), f=f_meas_vec(stance, prediction)) |>
  knitr::kable(digits=3)


```

# Per topic performance

The table below shows the f-score for each model per topic (macro-averaged over the stances including N),
with the best model per topic indicated in bold:

```{r pertopic}
p <- d |> group_by(model, shot, reason, topic) |> 
    summarize(n=n(), acc=mean(stance==prediction), f=f_meas_vec(stance, prediction)) |>
  ungroup() |>
  mutate(modelshot=str_c(model, shot, sep=":"), topic=fct_reorder(topic, f, .fun=mean)) |>
  group_by(topic) |>
  mutate(bestf=max(f)) 

ggplot(p, aes(x=modelshot, y=topic, fill=f, label=round(f, 2))) + geom_tile() + 
  geom_text(color="grey") +
  geom_text(data=filter(p, f==bestf), color="white") + 
  scale_fill_gradient(low="darkred", high="darkblue") + 
  theme_minimal() + theme(legend.position="none") + xlab("") + ylab("")
```

# Per class performance

Finally, the tables below give per-class precision/recall/f scores and confusion matrices for each invidual model:

```{r detailed, output='asis'}

class_measure <- function(actual, predicted, value, measure=f_meas_vec) {
  actual = factor(if_else(actual == value, "Y", "N"), levels=c("Y", "N"))
  predicted = factor(if_else(predicted == value, "Y", "N"),levels=c("Y", "N")) 
  measure(truth=actual, predicted, event_level="first")
}

model_plots <- function(model, shot, reason) {
  cat("\n\n")
  print(str_glue("\n\n## Model {model}: {shot} ({reason})\n"))
  cat("\n\n")
  
  p1 <- d |>  filter(model == .env$model, shot == .env$shot, reason == .env$reason) |>
    group_by(topic) |> 
    summarize( 
              L_Pr=class_measure(stance, prediction, "L", precision_vec),
              L_Re=class_measure(stance, prediction, "L", recall_vec),
              L_F1=class_measure(stance, prediction, "L", f_meas_vec),
              R_Pr=class_measure(stance, prediction, "R", precision_vec),
              R_Re=class_measure(stance, prediction, "R", recall_vec),
              R_F1=class_measure(stance, prediction, "R", f_meas_vec))  |>
    pivot_longer(-topic) |>
    separate(name, into=c("class", "measure"), sep = "_") |>
    mutate(measure=factor(measure, levels=c("Pr", "Re", "F1"))) |>
    ggplot(aes(x=measure, y=topic,  fill=value, label=round(value, 2))) + geom_tile() + 
    geom_text(color="grey") +
    #geom_text(data=filter(p, f==bestf), color="white") + 
    scale_fill_gradient(low="darkred", high="darkgreen") + 
    theme_minimal() + theme(legend.position="none") + 
    xlab("") + ylab("") + 
    facet_grid(cols=vars(class)) + 
    ggtitle(str_glue("Detailed performance for {model} {shot} ({reason})")) 
  
  print(p1)
  
  p2 <- d |>  filter(model == .env$model, shot == .env$shot, reason == .env$reason)  |>
    group_by(stance, prediction) |> 
    summarize(n=n()) |> ungroup() |>
    mutate(p=n/sum(n)) |>
    ggplot(aes(x=prediction, y=stance, fill=p, label=str_c(round(p*100, 1), "%"))) + 
    geom_tile() + 
    geom_text(color="white") +
    theme_minimal() +
    theme(legend.position="none") + xlab("GPT Prediction") + ylab("Human judgment") +
      ggtitle(str_glue("Confusion matrix for {model} {shot} ({reason})")) 
  
  print(p2) 
}

expand.grid(model=unique(d$model), shot=unique(d$shot), reason=unique(d$reason)) |>
  pwalk(model_plots)
```

